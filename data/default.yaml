# ============================================================================
# SFT Dataset Pipeline Configuration
# ============================================================================
# All dataset mixing weights, sampling strategies, quality gates, and
# view count distributions are controlled from this single file.
# ============================================================================

# --- Global Settings --------------------------------------------------------
global:
  output_dir: "./output/sft_dataset"
  seed: 42
  total_samples: 5000           # Target number of final SFT samples
  num_workers: 8                # Parallel workers for rendering / code execution
  blender_path: "blender"       # Path to Blender binary
  blender_timeout: 120          # Seconds before killing a Blender subprocess

# --- View Sampling ----------------------------------------------------------
# Controls how many views are provided as input for each SFT sample.
# Weights are relative (will be normalized to sum to 1.0).
views:
  min_views: 1
  max_views: 6
  count_weights:
    1: 0.20    # Single view — hardest, most common deployment case
    2: 0.20    # Two views
    3: 0.20    # Three views
    4: 0.20    # Four views
    5: 0.10    # Five views
    6: 0.10    # Six views
  # How to select WHICH views when fewer than max are used
  selection_strategy: "azimuth_spread"   # Options: azimuth_spread | random | fixed_canonical
  # For azimuth_spread: always pick maximally separated azimuth angles
  # For fixed_canonical: 1-view=front_3/4, 2-view=front+back, etc.
  elevation_jitter_deg: 10      # Random elevation variation in degrees
  render_resolution: [512, 512]

# --- Quality Gate -----------------------------------------------------------
# Samples must pass ALL of these criteria to be considered for selection.
quality_gate:
  min_reward: 0.10              # Minimum RLVR composite reward
  min_f_score_005: 0.08         # Minimum F-Score @ tau=0.05
  max_chamfer: 0.05             # Maximum Chamfer Distance
  min_code_length: 200          # Minimum characters in generated code
  max_code_length: 15000        # Maximum characters (avoid degenerate long outputs)
  require_geometry_ops: true    # Must contain actual geometry operations
  geometry_op_keywords:         # At least one of these must appear in code
    - "primitive_cube_add"
    - "primitive_cylinder_add"
    - "primitive_uv_sphere_add"
    - "primitive_cone_add"
    - "primitive_torus_add"
    - "from_pydata"
    - "boolean"
    - "bevel"
    - "solidify"
    - "array"
    - "extrude"
    - "curve"
    - "bezier"
    - "spin"
    - "screw"

# --- Dataset Sources --------------------------------------------------------
# Each source has a weight (relative share of final dataset), an adapter
# class, and source-specific configuration.
datasets:
  meshcoder:
    enabled: true
    weight: 0.40                # 40% of final dataset
    adapter: "MeshCoderAdapter"
    config:
      data_dir: "./data/meshcoder"
      train_json: "train.json"
      categories: null           # null = all categories; or list like ["chair", "table"]
      max_source_samples: null   # null = use all; or integer to cap
    # Difficulty distribution within this source
    difficulty_distribution:
      easy:      0.30            # reward 0.10 - 0.25
      medium:    0.35            # reward 0.25 - 0.45
      hard:      0.25            # reward 0.45 - 0.65
      excellent: 0.10            # reward 0.65+
    difficulty_thresholds:
      easy:      [0.10, 0.25]
      medium:    [0.25, 0.45]
      hard:      [0.45, 0.65]
      excellent: [0.65, 1.00]

  infinigen:
    enabled: true
    weight: 0.25                # 25% of final dataset
    adapter: "InfinigenAdapter"
    config:
      code_dir: "./data/infinigen/extracted_scripts"
      mesh_dir: "./data/infinigen/meshes"
      categories: null
      max_source_samples: null
    difficulty_distribution:
      easy:      0.40
      medium:    0.35
      hard:      0.20
      excellent: 0.05
    difficulty_thresholds:
      easy:      [0.10, 0.25]
      medium:    [0.25, 0.45]
      hard:      [0.45, 0.65]
      excellent: [0.65, 1.00]

  objaverse_llm:
    enabled: true
    weight: 0.25                # 25% of final dataset
    adapter: "ObjaverseLLMAdapter"
    config:
      data_dir: "./data/objaverse"
      lvis_categories: ["chair", "table", "lamp", "sofa", "bookshelf", "cabinet"]
      models_for_generation: ["grok-4.1", "claude-sonnet-4-20250514", "qwen-3.5-plus"]
      openrouter_api_key_env: "OPENROUTER_API_KEY"
      max_objects: 500           # How many unique objects to query
      max_retries: 2
      generation_timeout: 300
    difficulty_distribution:
      easy:      0.25
      medium:    0.35
      hard:      0.30
      excellent: 0.10
    difficulty_thresholds:
      easy:      [0.10, 0.25]
      medium:    [0.25, 0.45]
      hard:      [0.45, 0.65]
      excellent: [0.65, 1.00]

  shapenet:
    enabled: false               # Disabled by default
    weight: 0.10
    adapter: "ShapeNetAdapter"
    config:
      data_dir: "./data/shapenet"
      categories: ["chair", "table", "airplane", "car"]
      use_partnet_annotations: true
    difficulty_distribution:
      easy:      0.35
      medium:    0.35
      hard:      0.20
      excellent: 0.10
    difficulty_thresholds:
      easy:      [0.10, 0.25]
      medium:    [0.25, 0.45]
      hard:      [0.45, 0.65]
      excellent: [0.65, 1.00]

# --- Sampling Strategy ------------------------------------------------------
sampling:
  method: "stratified"          # Options: stratified | facility_location | reward_top_k
  
  # For stratified sampling:
  stratified:
    # Axes to stratify across (all are cross-producted)
    axes:
      - name: "category"
        source: "metadata.category"
      - name: "difficulty"
        source: "computed.difficulty_bucket"
      - name: "code_complexity"
        source: "computed.code_complexity_bucket"
        num_bins: 3              # simple, moderate, complex
    min_per_stratum: 1           # Minimum samples per non-empty stratum
    overflow_strategy: "drop_lowest_reward"  # How to trim if over budget
  
  # For facility_location (k-means based diverse selection):
  facility_location:
    feature_columns:
      - "num_primitives"
      - "uses_boolean"
      - "uses_array"
      - "uses_curves"
      - "uses_extrude"
      - "num_parts"
      - "code_length"
      - "num_functions"
      - "uses_math"
      - "reward"
      - "chamfer_distance"
      - "f_score_005"
    selection_within_cluster: "max_reward"

  # For reward_top_k (simplest — just take top K by reward):
  reward_top_k:
    ensure_min_per_category: 20  # Still enforce some category diversity

# --- Code Complexity Features -----------------------------------------------
# Used for stratification and facility_location clustering.
code_features:
  primitives_keywords:
    - "primitive_cube_add"
    - "primitive_cylinder_add"
    - "primitive_uv_sphere_add"
    - "primitive_cone_add"
    - "primitive_torus_add"
  boolean_keywords: ["boolean"]
  array_keywords: ["array", "for "]
  curve_keywords: ["curve", "bezier", "nurbs"]
  extrude_keywords: ["extrude"]
  math_keywords: ["import math", "sin(", "cos(", "radians(", "pi"]

# --- Output Format ----------------------------------------------------------
# Final SFT dataset format, compatible with Tinker / standard chat format.
output:
  format: "jsonl"               # Options: jsonl | parquet | huggingface
  chat_template: "chatml"       # Options: chatml | llama | qwen
  include_system_prompt: true
  system_prompt: |
    You are a 3D modeling assistant. Given one or more images of a 3D object,
    generate a Blender Python script that recreates the object's geometry.
    The script must: clear the scene, build geometry using bpy operations,
    normalize to a unit bounding box, and export as OBJ.
  # Whether to store metadata alongside each sample (useful for analysis)
  include_metadata: true
  metadata_fields:
    - "source_dataset"
    - "category"
    - "object_id"
    - "num_views"
    - "reward"
    - "f_score_005"
    - "chamfer_distance"
    - "code_complexity_bucket"
    - "difficulty_bucket"
  
  # DPO pairs output (optional — for storing rejected samples alongside chosen)
  generate_dpo_pairs: true
  dpo_output_dir: "./output/dpo_pairs"
  dpo_min_reward_gap: 0.15      # Minimum reward difference between chosen/rejected

# --- Logging & Monitoring ---------------------------------------------------
logging:
  level: "INFO"
  log_file: "./output/pipeline.log"
  report_file: "./output/dataset_report.json"
  # Generate distribution plots
  generate_plots: true
  plot_dir: "./output/plots"
